{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Módulo 4 - Big Data\n",
    "\n",
    "## Tarea 3\n",
    "\n",
    "Autor: Luis Felipe Cubero Vargas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+------+-------------------+\n",
      "| id|customer_id|amount|       purchased_at|\n",
      "+---+-----------+------+-------------------+\n",
      "|  1|          1|    55|2017-03-01 09:00:00|\n",
      "|  2|          1|   125|2017-03-01 10:00:00|\n",
      "|  3|          1|    32|2017-03-02 13:00:00|\n",
      "|  4|          1|    64|2017-03-02 15:00:00|\n",
      "|  5|          1|   128|2017-03-03 10:00:00|\n",
      "|  6|          2|   333|2017-03-01 09:00:00|\n",
      "|  7|          2|   334|2017-03-01 09:01:00|\n",
      "|  8|          2|   333|2017-03-01 09:02:00|\n",
      "|  9|          2|    11|2017-03-03 20:00:00|\n",
      "| 10|          2|    44|2017-03-03 20:15:00|\n",
      "+---+-----------+------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init('/opt/spark')\n",
    "\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, date_format, udf \n",
    "from pyspark.sql.types import DateType\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Basic JDBC pipeline\") \\\n",
    "    .config(\"spark.driver.extraClassPath\", \"./Tarea3/postgresql-42.2.8.jar\") \\\n",
    "    .config(\"spark.executor.extraClassPath\", \"./Tarea3/postgresql-42.2.8.jar\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Reading single DataFrame in Spark by retrieving all rows from a DB table.\n",
    "df = spark \\\n",
    "    .read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://localhost:5432/\") \\\n",
    "    .option(\"user\", \"postgres\") \\\n",
    "    .option(\"password\", \"root\") \\\n",
    "    .option(\"dbtable\", \"bigdata.transactions\") \\\n",
    "    .load()\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leer datos directamente rara vez es el objetivo primario. Las dos tareas típicas son: transformar algunas de las columnas en representaciones modificadas de las mismas, o bien, agregar grupos de filas en una. A manera de ejemplo, podemos asumir que se quiere la información del total de compras hechas por clientes.\n",
    "\n",
    "Spark provee ciertas transformaciones básicas. Por ejemplo, transformar fechas a una representación con formato específico, para poder truncar la una estampilla de tiempo a nivel de día. En este caso, se utiliza `date_format` en el módulo `pyspark.sql.functions`. El siguiente código crea una columna nueva después de aplicar la transformación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+------+-------------------+-----------+\n",
      "| id|customer_id|amount|       purchased_at|date_string|\n",
      "+---+-----------+------+-------------------+-----------+\n",
      "|  1|          1|    55|2017-03-01 09:00:00| 03/01/2017|\n",
      "|  2|          1|   125|2017-03-01 10:00:00| 03/01/2017|\n",
      "|  3|          1|    32|2017-03-02 13:00:00| 03/02/2017|\n",
      "|  4|          1|    64|2017-03-02 15:00:00| 03/02/2017|\n",
      "|  5|          1|   128|2017-03-03 10:00:00| 03/03/2017|\n",
      "|  6|          2|   333|2017-03-01 09:00:00| 03/01/2017|\n",
      "|  7|          2|   334|2017-03-01 09:01:00| 03/01/2017|\n",
      "|  8|          2|   333|2017-03-01 09:02:00| 03/01/2017|\n",
      "|  9|          2|    11|2017-03-03 20:00:00| 03/03/2017|\n",
      "| 10|          2|    44|2017-03-03 20:15:00| 03/03/2017|\n",
      "+---+-----------+------+-------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "formatted_df = df.withColumn(\"date_string\", date_format(col(\"purchased_at\"), 'MM/dd/yyyy'))\n",
    "formatted_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el caso que necesitemos crear una función que no es parte de la biblioteca estándar en Spark, es posible definir funciones creadas por el usuario (User Defined Functions o *udf*). La noción básica de una `udf` en Spark un lambda acompañado por el tipo de dato retornado. Lo anterior es estrictamente necesario en lenguajes con un sistema de tipos débil.\n",
    "\n",
    "Además, es una manera cómoda de encapsular funciones de Python que deben ser aplicadas a celdas del *Dataframe*, pero aún no tienen su implementación en las bibliotecas.\n",
    "\n",
    "El siguiente ejemplo muestra la columna creada previamente (tipo `string`) y la transforma en un tipo `DateType` propio de Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+------+-------------------+-----------+----------+\n",
      "| id|customer_id|amount|       purchased_at|date_string|      date|\n",
      "+---+-----------+------+-------------------+-----------+----------+\n",
      "|  1|          1|    55|2017-03-01 09:00:00| 03/01/2017|2017-03-01|\n",
      "|  2|          1|   125|2017-03-01 10:00:00| 03/01/2017|2017-03-01|\n",
      "|  3|          1|    32|2017-03-02 13:00:00| 03/02/2017|2017-03-02|\n",
      "|  4|          1|    64|2017-03-02 15:00:00| 03/02/2017|2017-03-02|\n",
      "|  5|          1|   128|2017-03-03 10:00:00| 03/03/2017|2017-03-03|\n",
      "|  6|          2|   333|2017-03-01 09:00:00| 03/01/2017|2017-03-01|\n",
      "|  7|          2|   334|2017-03-01 09:01:00| 03/01/2017|2017-03-01|\n",
      "|  8|          2|   333|2017-03-01 09:02:00| 03/01/2017|2017-03-01|\n",
      "|  9|          2|    11|2017-03-03 20:00:00| 03/03/2017|2017-03-03|\n",
      "| 10|          2|    44|2017-03-03 20:15:00| 03/03/2017|2017-03-03|\n",
      "+---+-----------+------+-------------------+-----------+----------+\n",
      "\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- amount: integer (nullable = true)\n",
      " |-- purchased_at: timestamp (nullable = true)\n",
      " |-- date_string: string (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "string_to_date = \\\n",
    "    udf(lambda text_date: datetime.strptime(text_date, '%m/%d/%Y'),\n",
    "        DateType())\n",
    "\n",
    "typed_df = formatted_df.withColumn(\"date\", string_to_date(formatted_df.date_string))\n",
    "typed_df.show()\n",
    "typed_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para sumar los datos podemos utilizar el concepto común de agrupamiento en SQL. Spark posee una agrupación de `groupBy`, directamente. En este caso, queremos sumar todas las compras por cliente y día:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-------+----------------+-----------+\n",
      "|customer_id|      date|sum(id)|sum(customer_id)|sum(amount)|\n",
      "+-----------+----------+-------+----------------+-----------+\n",
      "|          2|2017-03-01|     21|               6|       1000|\n",
      "|          1|2017-03-02|      7|               2|         96|\n",
      "|          1|2017-03-03|      5|               1|        128|\n",
      "|          1|2017-03-01|      3|               2|        180|\n",
      "|          2|2017-03-03|     19|               4|         55|\n",
      "+-----------+----------+-------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sum_df = typed_df.groupBy(\"customer_id\", \"date\").sum()\n",
    "sum_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark sumará todas las columnas **que no se encuentren** especificadas en la operación `groupBy`. Algunos resultados no tienen interpretación últil. Por ejemplo, sumar la columna `customer_id` no da ningún valor agregado. Finalmente, es posible dar a las columnas un nombre más amigable con los consumidores de los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- amount: long (nullable = true)\n",
      "\n",
      "+-----------+----------+------+\n",
      "|customer_id|      date|amount|\n",
      "+-----------+----------+------+\n",
      "|          2|2017-03-01|  1000|\n",
      "|          1|2017-03-02|    96|\n",
      "|          1|2017-03-03|   128|\n",
      "|          1|2017-03-01|   180|\n",
      "|          2|2017-03-03|    55|\n",
      "+-----------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stats_df = \\\n",
    "    sum_df.select(\n",
    "        col('customer_id'),\n",
    "        col('date'),\n",
    "        col('sum(amount)').alias('amount'))\n",
    "\n",
    "stats_df.printSchema()\n",
    "stats_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark permite cargar información de múltiples fuentes. A continuación se muestra como cargar datos de un archivo CSV que contiene los nombres de los clientes, así como la moneda en que realizan transacciones. Nótese que el CSV no tiene información de tipos de datos, por lo que es buena práctica agregarlos explícitamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- currency: string (nullable = true)\n",
      "\n",
      "+---+----+--------+\n",
      "| id|name|currency|\n",
      "+---+----+--------+\n",
      "|  1|John|     CRC|\n",
      "|  2|Jane|     EUR|\n",
      "+---+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType, StringType, StructField, StructType\n",
    "\n",
    "names_df = spark \\\n",
    "    .read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"path\", \"names.csv\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .schema(StructType([\n",
    "                StructField(\"id\", IntegerType()),\n",
    "                StructField(\"name\", StringType()),\n",
    "                StructField(\"currency\", StringType())])) \\\n",
    "    .load()\n",
    "\n",
    "names_df.printSchema()\n",
    "names_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez que la información fue cargada, la fuente particular no es relevante. A continuación se muestra como podemos enriquerecer la información utilizando la funcion **JOIN** entre *data frames*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- amount: long (nullable = true)\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- currency: string (nullable = true)\n",
      "\n",
      "+-----------+----------+------+---+----+--------+\n",
      "|customer_id|      date|amount| id|name|currency|\n",
      "+-----------+----------+------+---+----+--------+\n",
      "|          2|2017-03-01|  1000|  2|Jane|     EUR|\n",
      "|          1|2017-03-02|    96|  1|John|     CRC|\n",
      "|          1|2017-03-03|   128|  1|John|     CRC|\n",
      "|          1|2017-03-01|   180|  1|John|     CRC|\n",
      "|          2|2017-03-03|    55|  2|Jane|     EUR|\n",
      "+-----------+----------+------+---+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joint_df = stats_df.join(names_df, stats_df.customer_id == names_df.id)\n",
    "joint_df.printSchema()\n",
    "joint_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio\n",
    "- Cree un pequeño generador de transacciones. Para ello, puede utilizar funciones de numpy o pandas, para crear las transacciones nuevas en memoria y, posteriormente, puede cargarlas a un *Spark Dataframe* que después debe insertar en la base de datos.\n",
    "- Con los datos nuevos, ejecute el código de éste notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+------+-------------------+\n",
      "| id|customer_id|amount|       purchased_at|\n",
      "+---+-----------+------+-------------------+\n",
      "|  1|          1|    55|2017-03-01 09:00:00|\n",
      "|  2|          1|   125|2017-03-01 10:00:00|\n",
      "|  3|          1|    32|2017-03-02 13:00:00|\n",
      "|  4|          1|    64|2017-03-02 15:00:00|\n",
      "|  5|          1|   128|2017-03-03 10:00:00|\n",
      "|  6|          2|   333|2017-03-01 09:00:00|\n",
      "|  7|          2|   334|2017-03-01 09:01:00|\n",
      "|  8|          2|   333|2017-03-01 09:02:00|\n",
      "|  9|          2|    11|2017-03-03 20:00:00|\n",
      "| 10|          2|    44|2017-03-03 20:15:00|\n",
      "| 11|          1|    50|2017-03-04 21:15:00|\n",
      "+---+-----------+------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id', 'customer_id', 'amount', 'purchased_at']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+------+-------------------+\n",
      "| id|customer_id|amount|       purchased_at|\n",
      "+---+-----------+------+-------------------+\n",
      "|  1|          1|    55|2017-03-01 09:00:00|\n",
      "|  2|          1|   125|2017-03-01 10:00:00|\n",
      "|  3|          1|    32|2017-03-02 13:00:00|\n",
      "|  4|          1|    64|2017-03-02 15:00:00|\n",
      "|  5|          1|   128|2017-03-03 10:00:00|\n",
      "|  6|          2|   333|2017-03-01 09:00:00|\n",
      "|  7|          2|   334|2017-03-01 09:01:00|\n",
      "|  8|          2|   333|2017-03-01 09:02:00|\n",
      "|  9|          2|    11|2017-03-03 20:00:00|\n",
      "| 10|          2|    44|2017-03-03 20:15:00|\n",
      "| 11|          1|    50|2017-03-04 21:15:00|\n",
      "+---+-----------+------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newRow = spark.createDataFrame([(11,1,50,'2017-03-04 21:15:00')], df.columns)\n",
    "df = df.union(newRow)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "colab": {
   "default_view": {},
   "name": "ejemplo_latex.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
